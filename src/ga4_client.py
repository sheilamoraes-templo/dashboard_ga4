import json
import pandas as pd
from datetime import datetime, timedelta, date
from google.analytics.data_v1beta import BetaAnalyticsDataClient
from google.analytics.data_v1beta.types import (
    RunReportRequest,
    DateRange,
    Metric,
    Dimension,
    Filter,
    FilterExpression,
    FilterExpressionList,
    OrderBy
)
from config.settings import GA4_PROPERTY_ID, GA4_CREDENTIALS_PATH
from src.cache_manager import cache_manager
from .fake_data_client import FakeDataClient
from .superstore_data_client import SuperstoreDataClient

class GA4Client:
    def __init__(self):
        """Inicializa o cliente GA4"""
        self.property_id = GA4_PROPERTY_ID
        self.fake_client = None  # Carregar apenas quando necess√°rio
        self.superstore_client = None  # Carregar apenas quando necess√°rio
        
        # Tentar inicializar cliente GA4 real
        try:
            self.client = BetaAnalyticsDataClient.from_service_account_json(GA4_CREDENTIALS_PATH)
            self.use_fake_data = False
            print("‚úÖ Cliente GA4 real inicializado")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao inicializar GA4 real: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            self.client = None
            self.use_fake_data = True
            self.superstore_client = SuperstoreDataClient()
        
    def get_basic_metrics(self, days=30):
        """Obt√©m m√©tricas b√°sicas dos √∫ltimos N dias"""
        # Se estiver usando dados fake ou se a conex√£o GA4 falhar
        if self.use_fake_data or self.client is None:
            if self.superstore_client is None:
                self.superstore_client = SuperstoreDataClient()
            print("üîÑ Usando dataset Superstore para m√©tricas b√°sicas")
            return self.superstore_client.get_basic_metrics(days)
        
        try:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            
            request = RunReportRequest(
                property=f"properties/{self.property_id}",
                date_ranges=[DateRange(start_date=start_date, end_date=end_date)],
                metrics=[
                    Metric(name="totalUsers"),
                    Metric(name="sessions"),
                    Metric(name="screenPageViews"),
                    Metric(name="averageSessionDuration"),
                    Metric(name="bounceRate")
                ]
            )
            
            response = self.client.run_report(request)
            
            # Processar resposta
            data = {}
            for row in response.rows:
                for i, metric in enumerate(row.metric_values):
                    metric_name = request.metrics[i].name
                    data[metric_name] = metric.value
                    
            return data
            
        except Exception as e:
            print(f"Erro ao obter m√©tricas b√°sicas do GA4: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            if self.superstore_client is None:
                self.superstore_client = SuperstoreDataClient()
            return self.superstore_client.get_basic_metrics(days)
    
    def get_daily_metrics(self, days=30):
        """Obt√©m m√©tricas di√°rias dos √∫ltimos N dias"""
        # Se estiver usando dados fake ou se a conex√£o GA4 falhar
        if self.use_fake_data or self.client is None:
            print("üîÑ Usando dataset Superstore para m√©tricas di√°rias")
            return self.superstore_client.get_daily_metrics(days)
        
        try:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            
            request = RunReportRequest(
                property=f"properties/{self.property_id}",
                date_ranges=[DateRange(start_date=start_date, end_date=end_date)],
                metrics=[
                    Metric(name="totalUsers"),
                    Metric(name="sessions"),
                    Metric(name="screenPageViews")
                ],
                dimensions=[Dimension(name="date")]
            )
            
            response = self.client.run_report(request)
            
            # Converter para DataFrame
            data = []
            for row in response.rows:
                data.append({
                    'date': row.dimension_values[0].value,
                    'users': int(row.metric_values[0].value),
                    'sessions': int(row.metric_values[1].value),
                    'pageviews': int(row.metric_values[2].value)
                })
            
            return pd.DataFrame(data)
            
        except Exception as e:
            print(f"Erro ao obter m√©tricas di√°rias do GA4: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            return self.superstore_client.get_daily_metrics(days)
    
    def get_top_pages(self, days=30, limit=10):
        """Obt√©m p√°ginas mais visitadas"""
        # Se estiver usando dados fake ou se a conex√£o GA4 falhar
        if self.use_fake_data or self.client is None:
            print("üîÑ Usando dataset Superstore para p√°ginas mais visitadas")
            return self.superstore_client.get_top_pages(days, limit)
        
        try:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            
            request = RunReportRequest(
                property=f"properties/{self.property_id}",
                date_ranges=[DateRange(start_date=start_date, end_date=end_date)],
                metrics=[
                    Metric(name="screenPageViews"),
                    Metric(name="totalUsers"),
                    Metric(name="averageSessionDuration")
                ],
                dimensions=[
                    Dimension(name="pageTitle"),
                    Dimension(name="pagePath")
                ],
                limit=limit
            )
            
            response = self.client.run_report(request)
            
            # Converter para DataFrame
            data = []
            for row in response.rows:
                data.append({
                    'page_title': row.dimension_values[0].value,
                    'page_path': row.dimension_values[1].value,
                    'pageviews': int(row.metric_values[0].value),
                    'users': int(row.metric_values[1].value),
                    'avg_duration': float(row.metric_values[2].value)
                })
            
            return pd.DataFrame(data)
            
        except Exception as e:
            print(f"Erro ao obter p√°ginas mais visitadas do GA4: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            return self.superstore_client.get_top_pages(days, limit)
    
    def get_device_breakdown(self, days=30):
        """Obt√©m breakdown por dispositivo"""
        # Se estiver usando dados fake ou se a conex√£o GA4 falhar
        if self.use_fake_data or self.client is None:
            print("üîÑ Usando dataset Superstore para breakdown por dispositivo")
            return self.superstore_client.get_device_breakdown(days)
        
        try:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            
            request = RunReportRequest(
                property=f"properties/{self.property_id}",
                date_ranges=[DateRange(start_date=start_date, end_date=end_date)],
                metrics=[
                    Metric(name="totalUsers"),
                    Metric(name="sessions"),
                    Metric(name="screenPageViews")
                ],
                dimensions=[Dimension(name="deviceCategory")]
            )
            
            response = self.client.run_report(request)
            
            # Converter para DataFrame
            data = []
            for row in response.rows:
                data.append({
                    'device': row.dimension_values[0].value,
                    'users': int(row.metric_values[0].value),
                    'sessions': int(row.metric_values[1].value),
                    'pageviews': int(row.metric_values[2].value)
                })
            
            return pd.DataFrame(data)
            
        except Exception as e:
            print(f"Erro ao obter breakdown por dispositivo do GA4: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            return self.superstore_client.get_device_breakdown(days)
    
    def first_user_acquisition(self, days=30):
        """Obt√©m dados de aquisi√ß√£o de primeiro acesso"""
        if self.use_fake_data or self.client is None:
            print("üîÑ Usando dataset Superstore para aquisi√ß√£o de primeiro acesso")
            return self.superstore_client.first_user_acquisition(days)
        
        try:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            
            request = RunReportRequest(
                property=f"properties/{self.property_id}",
                date_ranges=[DateRange(start_date=start_date, end_date=end_date)],
                metrics=[
                    Metric(name="totalUsers"),
                    Metric(name="sessions")
                ],
                dimensions=[
                    Dimension(name="firstUserSource"),
                    Dimension(name="firstUserMedium"),
                    Dimension(name="firstUserCampaignName")
                ],
                limit=1000
            )
            
            response = self.client.run_report(request)
            
            # Converter para DataFrame
            data = []
            for row in response.rows:
                data.append({
                    'source': row.dimension_values[0].value,
                    'medium': row.dimension_values[1].value,
                    'campaign': row.dimension_values[2].value,
                    'users': int(row.metric_values[0].value),
                    'sessions': int(row.metric_values[1].value)
                })
            
            return pd.DataFrame(data)
            
        except Exception as e:
            print(f"Erro ao obter dados de aquisi√ß√£o do GA4: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            return self.superstore_client.first_user_acquisition(days)
    
    def video_events(self, days=30):
        """Obt√©m eventos de v√≠deo"""
        if self.use_fake_data or self.client is None:
            print("üîÑ Usando dataset Superstore para eventos de v√≠deo")
            if self.superstore_client is None:
                self.superstore_client = SuperstoreDataClient()
            return self.superstore_client.video_events(days)
        
        try:
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            
            request = RunReportRequest(
                property=f"properties/{self.property_id}",
                date_ranges=[DateRange(start_date=start_date, end_date=end_date)],
                metrics=[
                    Metric(name="eventCount")
                ],
                dimensions=[
                    Dimension(name="date"),
                    Dimension(name="eventName"),
                    Dimension(name="customEvent:video_title"),
                    Dimension(name="customEvent:video_percent")
                ],
                dimension_filter=FilterExpression(
                    filter=Filter(
                        field_name="eventName",
                        string_filter=Filter.StringFilter(
                            match_type=Filter.StringFilter.MatchType.CONTAINS,
                            value="video_"
                        )
                    )
                ),
                limit=10000
            )
            
            response = self.client.run_report(request)
            
            # Converter para DataFrame
            data = []
            for row in response.rows:
                data.append({
                    'date': row.dimension_values[0].value,
                    'event_name': row.dimension_values[1].value,
                    'video_title': row.dimension_values[2].value,
                    'video_percent': row.dimension_values[3].value,
                    'event_count': int(row.metric_values[0].value)
                })
            
            return pd.DataFrame(data)
            
        except Exception as e:
            print(f"Erro ao obter eventos de v√≠deo do GA4: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            if self.superstore_client is None:
                self.superstore_client = SuperstoreDataClient()
            return self.superstore_client.video_events(days)

    def test_connection(self):
        """Testa a conex√£o com GA4"""
        try:
            if self.use_fake_data or self.client is None:
                print("üîÑ Testando dataset Superstore...")
                return self.superstore_client.test_connection()
            
            # Tenta obter dados b√°sicos dos √∫ltimos 7 dias
            data = self.get_basic_metrics(days=7)
            if data:
                print("‚úÖ Conex√£o com GA4 estabelecida com sucesso!")
                print(f"Dados dos √∫ltimos 7 dias: {data}")
                return True
            else:
                print("‚ùå Falha na conex√£o com GA4")
                print("üîÑ Usando dataset Superstore como fallback")
                return self.superstore_client.test_connection()
        except Exception as e:
            print(f"‚ùå Erro ao testar conex√£o GA4: {e}")
            print("üîÑ Usando dataset Superstore como fallback")
            return self.superstore_client.test_connection()

    # ---------- Novos m√©todos para cat√°logo de relat√≥rios ----------
    
    def _build_filter_in(self, spec):
        """Constr√≥i filtro IN para dimens√µes"""
        if not spec: 
            return None
        return FilterExpression(
            filter=Filter(
                field_name=spec["dimension"],
                in_list_filter=Filter.InListFilter(values=list(spec.get("values", [])))
            )
        )

    def _build_filter_contains(self, spec):
        """Constr√≥i filtro CONTAINS para dimens√µes"""
        if not spec:
            return None
        return FilterExpression(
            filter=Filter(
                field_name=spec["dimension"],
                string_filter=Filter.StringFilter(
                    value=spec["contains"], 
                    match_type=Filter.StringFilter.MatchType.CONTAINS, 
                    case_sensitive=False
                )
            )
        )

    def _window(self, days: int):
        """Retorna tupla (start_date, end_date) para o per√≠odo especificado"""
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
        return start_date, end_date

    def _as_date(self, df: pd.DataFrame, column: str) -> pd.DataFrame:
        """Converte coluna para datetime se necess√°rio"""
        if column in df.columns and df[column].dtype == 'object':
            try:
                df[column] = pd.to_datetime(df[column], errors='coerce')
            except Exception:
                pass
        return df

    def _run_with_cache(self, method_name: str, days: int, **kwargs) -> pd.DataFrame:
        """Executa m√©todo com cache para reduzir chamadas √† API"""
        # Gerar chave de cache
        cache_params = {"days": days, **kwargs}
        cache_key = cache_manager.get_cache_key(method_name, cache_params)
        
        # Verificar se h√° cache v√°lido (30 minutos)
        if cache_manager.is_cache_valid(cache_key, max_age_minutes=30):
            cached_data = cache_manager.get_cached_data(cache_key)
            if cached_data and "data" in cached_data:
                print(f"üì¶ Usando cache para {method_name}")
                return pd.DataFrame(cached_data["data"])
        
        # Executar m√©todo original
        print(f"üîÑ Executando {method_name} via API...")
        method = getattr(self, method_name)
        result = method(days, **kwargs)
        
        # Salvar no cache
        if not result.empty:
            cache_manager.set_cached_data(cache_key, result.to_dict(orient="records"))
        
        return result

    def run_generic(self, days: int, dimensions: list, metrics: list,
                    filter_in: dict = None, filter_contains: dict = None,
                    order_by_metric: str = None, limit: int = 100000) -> pd.DataFrame:
        """Runner gen√©rico com filtros e ordena√ß√£o"""
        start, end = self._window(days)
        where = None
        
        # Combina filtros (AND) se ambos existirem
        f_in = self._build_filter_in(filter_in)
        f_ct = self._build_filter_contains(filter_contains)
        
        if f_in and f_ct:
            where = FilterExpression(
                and_group=FilterExpressionList(expressions=[f_in, f_ct])
            )
        else:
            where = f_in or f_ct

        rows, page_size, offset = [], min(limit, 100000), 0
        order_bys = []
        
        if order_by_metric:
            order_bys = [OrderBy(
                metric=OrderBy.MetricOrderBy(metric_name=order_by_metric), 
                desc=True
            )]

        while True:
            req = RunReportRequest(
                property=f"properties/{self.property_id}",
                dimensions=[Dimension(name=d) for d in dimensions],
                metrics=[Metric(name=m) for m in metrics],
                date_ranges=[DateRange(start_date=start, end_date=end)],
                limit=page_size, 
                offset=offset,
                dimension_filter=where, 
                order_bys=order_bys
            )
            
            resp = self.client.run_report(req)
            if not resp.rows: 
                break
                
            for r in resp.rows:
                row = {}
                for i, d in enumerate(dimensions):
                    row[d] = r.dimension_values[i].value
                for j, m in enumerate(metrics):
                    row[m] = r.metric_values[j].value
                rows.append(row)
                
            if len(resp.rows) < page_size or len(rows) >= limit: 
                break
            offset += page_size
            
        return pd.DataFrame(rows)

    def run_compare_periods(self, days: int, **kwargs):
        """Executa consulta comparando per√≠odo atual vs anterior"""
        # Per√≠odo atual: [today-days+1, today]
        # Per√≠odo anterior: imediatamente anterior, mesmo tamanho
        cur = self.run_generic(days=days, **kwargs)
        
        # Desloca a janela para tr√°s
        start_prev = (date.today() - timedelta(days=days*2)).isoformat()
        end_prev = (date.today() - timedelta(days=days)).isoformat()
        
        # Reusa o runner com datas espec√≠ficas
        prev = self._run(start_prev, end_prev, kwargs["dimensions"], kwargs["metrics"], 
                        limit=kwargs.get("limit", 100000))
        return cur, prev

    def postprocess(self, key: str, df: pd.DataFrame) -> pd.DataFrame:
        """P√≥s-processamento de dados baseado na chave"""
        if df is None or df.empty:
            return df

        # NUM√âRICOS padronizados
        for c in df.columns:
            if c in ["totalUsers", "sessions", "screenPageViews", "eventCount", "averageSessionDuration"]:
                df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)

        if key == "daily":
            df = self._as_date(df, "date")
            out = pd.DataFrame({
                "date": df["date"],
                "users": df["totalUsers"] if "totalUsers" in df else 0,
                "sessions": df["sessions"] if "sessions" in df else 0,
                "pageviews": df["screenPageViews"] if "screenPageViews" in df else 0,
            })
            return out.sort_values("date")

        if key == "pages":
            out = (df.groupby("pagePath", as_index=False)["screenPageViews"].sum()
                     .rename(columns={"pagePath": "page", "screenPageViews": "pageviews"})
                     .sort_values("pageviews", ascending=False))
            return out

        if key == "pages_compare":
            # Espera dois per√≠odos j√° combinados fora (veremos na rota); aqui s√≥ garantimos nomes
            return df  # deixamos a rota/refresh compor o comparativo

        if key == "devices":
            out = df.rename(columns={"deviceCategory": "device", "totalUsers": "users"})
            out["users"] = pd.to_numeric(out["users"], errors="coerce").fillna(0)
            return out.groupby("device", as_index=False)["users"].sum()

        if key == "first_user":
            out = df.rename(columns={
                "firstUserSource": "source",
                "firstUserMedium": "medium",
                "firstUserCampaignName": "campaign",
                "totalUsers": "users"
            })
            out["users"] = pd.to_numeric(out["users"], errors="coerce").fillna(0)
            return out.sort_values("users", ascending=False)

        if key == "days_top":
            df = self._as_date(df, "date")
            out = df.rename(columns={"totalUsers": "users"})[["date", "users"]]
            return out.sort_values("users", ascending=False).head(30)

        if key == "weekday_heatmap":
            df = self._as_date(df, "date")
            if "totalUsers" in df:
                df = df.rename(columns={"totalUsers": "users"})
            df["dow"] = df["date"].dt.dayofweek  # 0=Seg ... 6=Dom
            df["week"] = df["date"].dt.isocalendar().week
            out = df.groupby(["week", "dow"], as_index=False)["users"].sum()
            # Tamb√©m devolve vers√£o "leg√≠vel"
            out["day_name"] = out["dow"].map({0: "Seg", 1: "Ter", 2: "Qua", 3: "Qui", 4: "Sex", 5: "S√°b", 6: "Dom"})
            return out.sort_values(["week", "dow"])

        if key == "compare_sum":
            # Ser√° montado fora (somat√≥rio cur vs prev); aqui s√≥ dizemos que √© comparativo
            return df

        if key == "compare_avg_duration":
            return df

        return df

    def video_events_specific(self, days: int, event_names: list) -> pd.DataFrame:
        """Eventos de v√≠deo espec√≠ficos por tipo"""
        start, end = self._window(days)
        ev_filter = FilterExpression(
            filter=Filter(
                field_name="eventName",
                in_list_filter=Filter.InListFilter(values=event_names)
            )
        )
        
        dims_base = ["date", "eventName"]
        mets = ["eventCount"]
        title_candidates = ["videoTitle", "customEvent:video_title", "customEvent:title"]
        percent_candidates = ["percent", "videoPercent", "customEvent:percent", "customEvent:video_percent"]

        for title_dim in title_candidates:
            for perc_dim in percent_candidates:
                try:
                    df = self._run(start, end, dims_base + [title_dim, perc_dim], mets, 
                                 limit=1_000_000, where=ev_filter)
                    if not df.empty:
                        df = self._as_date(df, "date")
                        df["event_count"] = pd.to_numeric(df["eventCount"], errors="coerce").fillna(0)
                        return df.rename(columns={
                            title_dim: "video_title",
                            perc_dim: "video_percent",
                            "eventName": "event_name"
                        })[["date", "event_name", "video_title", "video_percent", "event_count"]]
                except Exception:
                    continue
                    
        # Fallback sem t√≠tulo/percent
        df = self._run(start, end, ["date", "eventName"], mets, limit=1_000_000, where=ev_filter)
        if df.empty: 
            return pd.DataFrame(columns=["date", "event_name", "video_title", "video_percent", "event_count"])
            
        df = self._as_date(df, "date")
        df["event_count"] = pd.to_numeric(df["eventCount"], errors="coerce").fillna(0)
        df = df.rename(columns={"eventName": "event_name"})
        df["video_title"] = None
        df["video_percent"] = None
        return df[["date", "event_name", "video_title", "video_percent", "event_count"]]
