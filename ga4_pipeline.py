"""
Pipeline GA4 - Download e Processamento de Dados
Sistema completo para baixar dados espec√≠ficos do GA4 e disponibilizar no dashboard
"""

import os
import sys
import pandas as pd
from datetime import datetime, timedelta
import logging

# Adicionar src ao path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

try:
    from ga4_client import GA4Client
    from data_processor import data_processor
except ImportError as e:
    print(f"‚ùå Erro ao importar m√≥dulos: {e}")
    print("üîß Verifique se os arquivos est√£o na pasta src/")
    sys.exit(1)

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GA4Pipeline:
    """Pipeline completo para download e processamento de dados GA4"""
    
    def __init__(self):
        self.ga4_client = None
        self.data_dir = "data"
        self.ensure_data_dir()
        
    def ensure_data_dir(self):
        """Garante que a pasta data existe"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            logger.info(f"üìÅ Pasta {self.data_dir} criada")
    
    def initialize_ga4_client(self):
        """Inicializa o cliente GA4"""
        try:
            self.ga4_client = GA4Client()
            logger.info("‚úÖ Cliente GA4 inicializado")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar GA4: {e}")
            return False
    
    def download_main_metrics(self, days=30):
        """Baixa m√©tricas principais (usu√°rios, sess√µes, pageviews, etc.)"""
        logger.info(f"üìä Baixando m√©tricas principais para {days} dias...")
        
        try:
            # Baixar dados b√°sicos
            metrics_dict = self.ga4_client.get_basic_metrics(days=days)
            
            if metrics_dict and isinstance(metrics_dict, dict):
                # Converter dict para DataFrame
                df = pd.DataFrame([metrics_dict])
                
                # Processar dados
                df_processed = data_processor.process_dataframe(df, "kpis_daily")
                
                # Salvar CSV
                filename = "kpis_daily.csv"
                filepath = os.path.join(self.data_dir, filename)
                df_processed.to_csv(filepath, index=False)
                
                logger.info(f"‚úÖ M√©tricas principais salvas: {filename} ({len(df_processed)} registros)")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum dado de m√©tricas principais encontrado")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar m√©tricas principais: {e}")
            return False
    
    def download_top_pages(self, days=30, limit=50):
        """Baixa top p√°ginas e links mais acessados"""
        logger.info(f"üìÑ Baixando top {limit} p√°ginas para {days} dias...")
        
        try:
            # Baixar dados de p√°ginas
            df = self.ga4_client.get_top_pages(days=days, limit=limit)
            
            if df is not None and not df.empty:
                # Processar dados
                df_processed = data_processor.process_dataframe(df, "pages_top")
                
                # Salvar CSV
                filename = "pages_top.csv"
                filepath = os.path.join(self.data_dir, filename)
                df_processed.to_csv(filepath, index=False)
                
                logger.info(f"‚úÖ Top p√°ginas salvas: {filename} ({len(df_processed)} registros)")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum dado de p√°ginas encontrado")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar top p√°ginas: {e}")
            return False
    
    def download_device_breakdown(self, days=30):
        """Baixa breakdown por dispositivo"""
        logger.info(f"üì± Baixando breakdown por dispositivo para {days} dias...")
        
        try:
            # Baixar dados de dispositivos
            df = self.ga4_client.get_device_breakdown(days=days)
            
            if df is not None and not df.empty:
                # Processar dados
                df_processed = data_processor.process_dataframe(df, "devices")
                
                # Salvar CSV
                filename = "devices.csv"
                filepath = os.path.join(self.data_dir, filename)
                df_processed.to_csv(filepath, index=False)
                
                logger.info(f"‚úÖ Breakdown por dispositivo salvo: {filename} ({len(df_processed)} registros)")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum dado de dispositivos encontrado")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar breakdown por dispositivo: {e}")
            return False
    
    def download_first_user_acquisition(self, days=30):
        """Baixa dados de primeiros acessos (source/medium)"""
        logger.info(f"üéØ Baixando primeiros acessos para {days} dias...")
        
        try:
            # Baixar dados de aquisi√ß√£o
            df = self.ga4_client.get_first_user_acquisition(days=days)
            
            if df is not None and not df.empty:
                # Processar dados
                df_processed = data_processor.process_dataframe(df, "acquisition")
                
                # Salvar CSV
                filename = "first_user_acquisition.csv"
                filepath = os.path.join(self.data_dir, filename)
                df_processed.to_csv(filepath, index=False)
                
                logger.info(f"‚úÖ Primeiros acessos salvos: {filename} ({len(df_processed)} registros)")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum dado de primeiros acessos encontrado")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar primeiros acessos: {e}")
            return False
    
    def download_video_events(self, days=30):
        """Baixa eventos de v√≠deo (video_start, video_progress, video_complete)"""
        logger.info(f"üé¨ Baixando eventos de v√≠deo para {days} dias...")
        
        try:
            # Baixar dados de eventos de v√≠deo
            df = self.ga4_client.get_video_events(days=days)
            
            if df is not None and not df.empty:
                # Processar dados
                df_processed = data_processor.process_dataframe(df, "video_events")
                
                # Salvar CSV
                filename = "video_events.csv"
                filepath = os.path.join(self.data_dir, filename)
                df_processed.to_csv(filepath, index=False)
                
                logger.info(f"‚úÖ Eventos de v√≠deo salvos: {filename} ({len(df_processed)} registros)")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum evento de v√≠deo encontrado")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar eventos de v√≠deo: {e}")
            return False
    
    def download_weekly_comparison(self, weeks=4):
        """Baixa dados para compara√ß√£o semanal"""
        logger.info(f"üìÖ Baixando compara√ß√£o semanal para {weeks} semanas...")
        
        try:
            # Baixar dados semanais
            df = self.ga4_client.get_weekly_comparison(weeks=weeks)
            
            if df is not None and not df.empty:
                # Processar dados
                df_processed = data_processor.process_dataframe(df, "weekly_comparison")
                
                # Salvar CSV
                filename = "weekly_comparison.csv"
                filepath = os.path.join(self.data_dir, filename)
                df_processed.to_csv(filepath, index=False)
                
                logger.info(f"‚úÖ Compara√ß√£o semanal salva: {filename} ({len(df_processed)} registros)")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum dado de compara√ß√£o semanal encontrado")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar compara√ß√£o semanal: {e}")
            return False
    
    def download_days_with_most_users(self, days=30):
        """Baixa dados dos dias com mais usu√°rios"""
        logger.info(f"üìà Baixando dias com mais usu√°rios para {days} dias...")
        
        try:
            # Baixar dados di√°rios detalhados
            df = self.ga4_client.get_days_with_most_users(days=days)
            
            if df is not None and not df.empty:
                # Processar dados
                df_processed = data_processor.process_dataframe(df, "days_with_most_users")
                
                # Salvar CSV
                filename = "days_with_most_users.csv"
                filepath = os.path.join(self.data_dir, filename)
                df_processed.to_csv(filepath, index=False)
                
                logger.info(f"‚úÖ Dias com mais usu√°rios salvos: {filename} ({len(df_processed)} registros)")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum dado de dias com mais usu√°rios encontrado")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar dias com mais usu√°rios: {e}")
            return False
    
    def run_full_pipeline(self, days=30):
        """Executa o pipeline completo"""
        logger.info("üöÄ Iniciando pipeline completo de dados GA4...")
        
        # Inicializar cliente GA4
        if not self.initialize_ga4_client():
            logger.error("‚ùå Falha ao inicializar cliente GA4. Pipeline interrompido.")
            return False
        
        # Lista de downloads
        downloads = [
            ("M√©tricas Principais", lambda: self.download_main_metrics(days)),
            ("Top P√°ginas", lambda: self.download_top_pages(days)),
            ("Breakdown por Dispositivo", lambda: self.download_device_breakdown(days)),
            ("Primeiros Acessos", lambda: self.download_first_user_acquisition(days)),
            ("Eventos de V√≠deo", lambda: self.download_video_events(days)),
            ("Compara√ß√£o Semanal", lambda: self.download_weekly_comparison()),
            ("Dias com Mais Usu√°rios", lambda: self.download_days_with_most_users(days))
        ]
        
        # Executar downloads
        success_count = 0
        total_count = len(downloads)
        
        for name, download_func in downloads:
            logger.info(f"üì• Executando: {name}")
            try:
                if download_func():
                    success_count += 1
                    logger.info(f"‚úÖ {name} - Conclu√≠do")
                else:
                    logger.warning(f"‚ö†Ô∏è {name} - Falhou")
            except Exception as e:
                logger.error(f"‚ùå {name} - Erro: {e}")
        
        # Resumo final
        logger.info(f"üéâ Pipeline conclu√≠do: {success_count}/{total_count} downloads bem-sucedidos")
        
        if success_count > 0:
            logger.info("üìä Dados dispon√≠veis no dashboard Streamlit!")
            logger.info("üåê Execute: streamlit run streamlit_dashboard.py")
        
        return success_count > 0
    
    def run_quick_pipeline(self, days=7):
        """Executa pipeline r√°pido com dados essenciais"""
        logger.info("‚ö° Iniciando pipeline r√°pido...")
        
        if not self.initialize_ga4_client():
            return False
        
        # Downloads essenciais
        essential_downloads = [
            ("M√©tricas Principais", lambda: self.download_main_metrics(days)),
            ("Top P√°ginas", lambda: self.download_top_pages(days)),
            ("Breakdown por Dispositivo", lambda: self.download_device_breakdown(days))
        ]
        
        success_count = 0
        for name, download_func in essential_downloads:
            if download_func():
                success_count += 1
        
        logger.info(f"‚ö° Pipeline r√°pido conclu√≠do: {success_count}/{len(essential_downloads)} downloads")
        return success_count > 0

def main():
    """Fun√ß√£o principal para execu√ß√£o do pipeline"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Pipeline GA4 - Download de Dados")
    parser.add_argument("--days", type=int, default=30, help="N√∫mero de dias para baixar (padr√£o: 30)")
    parser.add_argument("--quick", action="store_true", help="Executar pipeline r√°pido (7 dias)")
    parser.add_argument("--full", action="store_true", help="Executar pipeline completo")
    
    args = parser.parse_args()
    
    # Criar pipeline
    pipeline = GA4Pipeline()
    
    if args.quick:
        # Pipeline r√°pido
        success = pipeline.run_quick_pipeline(days=7)
    elif args.full:
        # Pipeline completo
        success = pipeline.run_full_pipeline(days=args.days)
    else:
        # Pipeline padr√£o (m√©tricas principais + top p√°ginas)
        logger.info("üìä Executando pipeline padr√£o...")
        if pipeline.initialize_ga4_client():
            success = (
                pipeline.download_main_metrics(args.days) and
                pipeline.download_top_pages(args.days) and
                pipeline.download_device_breakdown(args.days)
            )
        else:
            success = False
    
    if success:
        logger.info("üéâ Pipeline executado com sucesso!")
        logger.info("üåê Execute o dashboard: streamlit run streamlit_dashboard.py")
    else:
        logger.error("‚ùå Pipeline falhou. Verifique as credenciais GA4.")

if __name__ == "__main__":
    main()
